---
title: "Resumen"
subtitle: "R data analisis"
linktitle: "Resumen"
date: "2024-05-07"
lang: es
---

# Presentación

## Objetivo de la práctica

Como siempre, estas guías no apuntan a ser un **paso a paso** de lo que se debe hacer, sino que son un ejemplo concreto de cómo hacer ciertos tipos de análisis con un conjunto de datos determinados. Los procedimientos de limpieza van a depender siempre del conjunto de datos que utilicemos en nuestras investigaciones y el proceso de análisis va a depender de nuestros objetivos de investigación, en relación con las variables e hipótesis que queramos demostrar.

El desarrollo de esta guía tiene por objetivo revisar todos los contenidos trabajados hasta este momento del curso (07/05/2024). En términos generales, es un resumen de las cosas esenciales de los 7 prácticos anteriores como limpieza y preparación de datos, análisis descriptivo, bivariado y visualización de datos en Quarto y subida a github y visualización de github pages.

En concreto, un ejemplo del resultado final de un trabajo se debería poder visualizar así en un [repositorio de github](https://github.com/Kevin-carrasco/ipo) y en [Github pages](https://kevin-carrasco.github.io/ipo/trabajo.html)

Por temas de orden y reproducibilidad, en este curso hemos separado en **dos momentos** el trabajo con datos, y dos archivos de código correspondientes:

  - **Preparación** corresponde a lo que se conoce generalmente como "limpieza", es decir, realizar las modificaciones necesarias para poder efectuar los análisis. Estas modificaciones previas al análisis son necesarias ya que los datos originales con los que se va a trabajar en general no vienen perfectamente adaptados a los análisis que se quieren hacer. Por lo tanto, en cuanto a datos también hacemos la distinción entre datos originales y datos preparados (o procesados). Este código de análisis lo podemos encontrar en la carpeta [procesamiento](https://github.com/Kevin-carrasco/ipo/tree/main/processing) del repositorio

  - **Análisis**: se relaciona tanto con análisis descriptivos asociados a las preguntas de investigación y como también modelamiento de datos para contrastar hipótesis de investigación. En este curso lo esencial es visualizar el análisis a través de quarto, por lo que el [documento .qmd](https://github.com/Kevin-carrasco/ipo/blob/main/trabajo.qmd) debería estar en el inicio del repositorio (junto al Rproject). Este documento se debe renderizar y obtenemos un [archivo .html](https://github.com/Kevin-carrasco/ipo/blob/main/trabajo.html) que se podría visualizar a través de github pages en, por ejemplo, el link: [https://kevin-carrasco.github.io/ipo/trabajo.html](https://kevin-carrasco.github.io/ipo/trabajo.html). Dónde:
  
![](../files/img/url-pages.png)

**Vamos paso a paso**

# Preparación

## Los procesos de preparación y análisis vinculados a datos y resultados se presentan en el siguiente esquema:{#flujo}

![](images/produccion.png)

Tanto la preparación como el análisis (que son parte del concepto más general de procesamiento) quedan registrados cada uno en un _archivo de código_.

<span class="sidenote">**Archivo de código R**: archivo con extensión .R donde se almacena el código de análisis. Para generarlo desde RStudio: _File > New File > R Script_ (o ctrl+shift+N), y para grabarlo  _File > Save (o ctrl+s)_, y darle nombre la primera vez (recordar: sin tilde ni ñ, y evitar espacios) </span>

El documento de **código de preparación** posee 5 partes, más una sección de identificación inicial:

0. Identificación y descripción general: Título, autor(es), fecha, información breve sobre el contenido del documento
1. Librerías: cargar librerías a utilizar
2. Datos: carga de datos
3. Selección de variables a utilizar
4. Procesamiento de variables: en este punto, por cada variable se realiza lo siguiente:
    a. Descriptivo básico
    b. Recodificación: datos perdidos y valores (en caso de ser necesario)
    c. Etiquetamiento: de variable y valores (en caso de ser necesario)
    e. Otros ajustes


5. Generación de base de datos preparada para el análisis.

<div class="alert alert-info">
**De rutas, estructura de carpetas y otros **

- **Encontrando la ruta a carpeta local**: lo más fácil es crear la carpeta donde se desean guardar los datos desde el administrador de archivos del computador. Luego, posicionarse con el cursor sobre la carpeta y seleccionar "Propiedades", en la ventana emergente debería aparecer la ruta hacia la carpeta en "Ubicación". Copiar esa ruta y agregar al final el nombre de la carpeta (separada por slash)

- **Sobre los "slashes" (`\` o `/`)**: en la ruta las carpetas y el archivo final aparecen separados por slashes, que según el sistema utilizado pueden ser _slash_ (`/`) o _backslash_ (`\`). En R por defecto se usa _slash_, pero en Windows _backslash_, por lo que si se usa Windows hay que reemplazarlos por _backslash_ o también puede ser por un doble _slash_ (`//`).

- Por temas de compatibilidad general, en las rutas se recomienda evitar tildes, eñes, espacios, mayúsculas y guiones bajos (_).

- **Estructura de carpetas**: para mantener el orden se sugiere seguir un protocolo de estructura de carpetas de proyecto, para lo que recomendamos el protocolo [IPO](https://lisa-coes.com/ipo-protocol/), y que se adapta al flujo de trabajo presentado anteriormente. Básicamente son tres carpetas: **input**, **procesamiento**, **output**. En la carpeta input crear la subcarpeta data-orig para guardar datos originales, y data-proc para los procesados. En procesamiento se guardan los archivos de código y en output las tablas y los gráficos.

![](../files/img/ipo-hex.png)


<div>

Nuestras carpetas se deberían ver así:

![](../files/img/carpetas.png)

Donde lo primero que debemos hacer, siempre, es abrir el Rproject (.Rproj) para comenzar nuestro trabajo.

## Antecedentes de los datos a utilizar

Cohesión barrial con elsoc 2016



## 1. Librerías principales (de R) a utilizar en el análisis{#librerias}

```{r}
pacman::p_load(dplyr, sjmisc, car, sjlabelled, stargazer, haven, sjPlot, ggplot2, psych)
```

## 2. Cargar base de datos

**Ajustar espacio de trabajo**

Previo a la carga de nuestra base de datos, se recomienda ejecutar los siguientes comandos:

```{r}
rm(list=ls())       # borrar todos los objetos en el espacio de trabajo
options(scipen=999) # valores sin notación científica
```

La función `rm(list=ls())` permite comenzar con un espacio de trabajo (environment) vacío y sin otros objetos. Así también, la función `options(scipen=999)` desactiva la notación científica, es decir, veremos los valores numéricos con todos sus decimales.

**Datos**

Las bases de datos se pueden cargar de un archivo local o en línea. Para este caso utilizaremos un archivo en línea que viene en formato RData: **elsoc.RData**. <span class="sidenote">**Abrir bases de datos en otros formatos**: Los formatos mas comunes en que se almacenan las bases de datos son .dta (Stata),  .sav (Spss) y RData (R). Para abrir desde R utlilizamos la librería `haven` y sus funciones read_dta y read_sav según corresponda. Ej: `datos <- read_dta("base_casen.dta")`. Recordar antes instalar/cargar la librería: `pacman::p_load(haven)`   </span>

```{r}
#cargamos la base de datos desde internet
load(url("https://dataverse.harvard.edu/api/access/datafile/7245118")) #Cargar base de datos
```


Realizamos un chequeo básico de la lectura de datos: nombres de las variables y tamaño de la base en términos de casos y variables (en este ejemplo, `r dim(elsoc_long_2016_2022.2)` ).

```{r}
dim(elsoc_long_2016_2022.2) # dimension de la base
```

## 3. Selección de variables a utilizar

Este paso consiste en crear un subset reducido de datos que contenga solo las variables de interés. Para ello lo más fácil es revisar el libro de códigos de cada base de datos. Además filtramos por la ola 1 para trabajar solo con datos del 2016.

```{r}
proc_data <- elsoc_long_2016_2022.2 %>% filter(ola=="1") %>% 
  select(t02_01, # Este barrio es ideal para mi
         t02_02, # Me siento incluido en este barrio
         t02_03, # Me identifico con la gente de este barrio
         t02_04, # Este barrio es parte de mi
         m01,# nivel educacional
         m0_sexo,# sexo
         m0_edad# edad
         )

# Comprobar
names(proc_data)
```

Mediante el comando `get_label` obtenemos el atributo label de las variables.

```{r}
sjlabelled::get_label(proc_data)
```

Podemos ver que son largas o con códigos poco informativos, por lo tanto, es necesario cambiarlas por etiquetas más cortas y de fácil identificación.

## Procesamiento de variables

Para el procesamiento de cada variable se seguirá el siguiente flujo de trabajo:

a. Descriptivo general
b. Recodificación: de casos perdidos y otros valores (en caso necesario)
c. Etiquetado: cambio de nombres de variables y valores (en caso necesario)
d. Otros ajustes

Y se recomienda también un descriptivo final para revisar que el procesamiento de cada variable está ok.

### cohesión barrial

_a. Descriptivo_

Para los descriptivos se utilizará la función `frq`, de la librería `sjmisc`:

```{r}
frq(proc_data$t02_01)
```

En esta variable vemos valores asociados a la opción "No contesta" (-999) y "No sabe" (-888), (-777) y (-666) que corresponde definirlos como casos perdidos (en el caso de R, como casos NA). El resto de los valores y etiquetas se encuentran en un orden correcto. Sin embargo, si queremos construir una escala, lo mejor es dejar los valores de 0 a 4

_b. Recodificación_

Después de revisar el libro de códigos, no hay variables en que los valores negativos representen alguna otra característica, así que podemos usar set_na

```{r }
proc_data <- proc_data %>% set_na(., na = c(-999, -888, -777, -666))
```

```{r}
frq(proc_data$t02_01)
```


Para reordenar las categorías volvemos a utilizar la función `recode`, de la librería `car`

```{r}
proc_data$t02_01 <- recode(proc_data$t02_01, "1=0; 2=1; 3=2; 4=3; 5=4")
proc_data$t02_02 <- recode(proc_data$t02_02, "1=0; 2=1; 3=2; 4=3; 5=4")
proc_data$t02_03 <- recode(proc_data$t02_03, "1=0; 2=1; 3=2; 4=3; 5=4")
proc_data$t02_04 <- recode(proc_data$t02_04, "1=0; 2=1; 3=2; 4=3; 5=4")
```

_c - Etiquetado_

Vamos a dar un nombre más sustantivo a las variables con la función `rename`, de la librería `dplyr`:

```{r}
proc_data <- proc_data %>% rename("ideal"=t02_01, 
                                  "integracion"=t02_02, 
                                  "identificacion"=t02_03, 
                                  "pertenencia"=t02_04)

```


Además de cambiar el nombre, queremos cambiar las etiquetas de las variables.

```{r}
proc_data$ideal <- set_label(x = proc_data$ideal,label = "Este barrio es ideal para mi")
get_label(proc_data$ideal)

proc_data$integracion  <- set_label(x = proc_data$integracion, label = "Me siento integrado en este barrio")
get_label(proc_data$integracion)

proc_data$identificacion  <- set_label(x = proc_data$identificacion, label = "Me identifico con la gente de este barrio")
get_label(proc_data$identificacion)

proc_data$pertenencia  <- set_label(x = proc_data$pertenencia, label = "Me siento parte de este barrio")
get_label(proc_data$pertenencia)
```

_Revisión final_

Nuevamente un descriptivo de cada variable para confirmar que el procesamiento está ok:

```{r}
frq(proc_data$ideal)
frq(proc_data$integracion)
frq(proc_data$identificacion)
frq(proc_data$pertenencia)
```

Vemos que los valores (labels) de cada categoría de las variables que recodificamos no se corresponden con el nuevo valor. Para re-etiquetar valores usamos la función `set_labels`, de la librería `sjlabelled`

```{r}
proc_data$ideal <- set_labels(proc_data$ideal,
            labels=c( "Totalmente en desacuerdo"=0,
                      "En desacuerdo"=1,
                      "Ni de acuerdo ni en desacuerdo"=2,
                      "De acuerdo"=3,
                      "Totalmente de acuerdo"=4))

proc_data$integracion <- set_labels(proc_data$integracion,
            labels=c( "Totalmente en desacuerdo"=0,
                      "En desacuerdo"=1,
                      "Ni de acuerdo ni en desacuerdo"=2,
                      "De acuerdo"=3,
                      "Totalmente de acuerdo"=4))

proc_data$identificacion <- set_labels(proc_data$identificacion,
            labels=c( "Totalmente en desacuerdo"=0,
                      "En desacuerdo"=1,
                      "Ni de acuerdo ni en desacuerdo"=2,
                      "De acuerdo"=3,
                      "Totalmente de acuerdo"=4))

proc_data$pertenencia <- set_labels(proc_data$pertenencia,
            labels=c( "Totalmente en desacuerdo"=0,
                      "En desacuerdo"=1,
                      "Ni de acuerdo ni en desacuerdo"=2,
                      "De acuerdo"=3,
                      "Totalmente de acuerdo"=4))
```

y volvemos a revisar

```{r}
frq(proc_data$ideal)
frq(proc_data$pertenencia)
```

#### 4.2. Educación

* [`m01`] =  Nivel de estudios alcanzado - Entrevistado 

_a. Descriptivo_

```{r}
frq(proc_data$m01)
```

Esta vez la vamos a dejar así

### 4.3. Sexo

* [`m0_sexo`]	=	SEXO Sexo

_a. Descriptivo_

```{r}
frq(proc_data$m0_sexo)
```



### 4.4 Edad

* [`m0_edad`]	=	EDAD Edad.


_a. Descriptivo_

```{r}
summary(proc_data$m0_edad)
```


## 5. Generación de base de datos procesada para el análisis

Antes de guardar la base procesada, revisamos nuevamente todas las variables con una tabla descriptiva general mediante la función `stargazer` (de la librería homónima)

Primero vamos a reformatear el objeto proc_data como base de datos (as.data.frame), paso necesario para que sea reconocido como tal por `stargazer`


```{r}
proc_data <-as.data.frame(proc_data)
stargazer(proc_data, type="text")
```


- Guardar base de datos procesada: en carpeta local <span class="sidenote">La ruta hacia su carpeta local si está trabajando en windows debería ser algo como "C:/Users/Lenovo/Clases/y aquí nombre del archivo a grabar</span>

El comando para guardar es `save`:

```{r eval=FALSE}
save(proc_data,file = "[ruta hacia carpeta local en su computador]/ELSOC_ess_merit2016.RData")
```

En este caso, seguimos una estructura de carpetas de datos, separando en una carpeta los datos originales, y en otra (proc) los datos procesados:

```{r eval=FALSE}
save(proc_data,file = "input/data/elsoc2016_proc.RData")
```

# Análisis

Una vez que tenemos recodificadas nuestras variables en el archivo de preparación y logramos exportar la base de datos procesada en la carpeta input/data, abrimos un documento de quarto (.qmd) para realizar el análisis.

Al trabajar con quarto (y al intentar renderizar), el documento leerá todos lo que esté escrito en el documento desde 0, por lo que es necesario **siempre** cargar de nuevo los paquetes y bases de datos.

Primero cargamos los paquetes:

```{r}
pacman::p_load(dplyr, sjmisc, car, sjlabelled, stargazer, haven, sjPlot, ggplot2, psych, kableExtra, corrplot)
```

y la base procesada

```{r eval=FALSE}
load("input/data/elsoc2016_proc.RData")
```

### Análisis descriptivo

```{r}
#| label: tbl-sjmisc
#| tbl-cap: "Descriptivos"

sjmisc::descr(proc_data,
      show = c("label","range", "mean", "sd", "NA.prc", "n"))%>% # Selecciona estadísticos
      kable(.,"markdown") # Esto es para que se vea bien en quarto
```

En la @tbl-sjmisc podemos observar los descriptivos generales de la base de datos procesada.

Y si queremos visualizar algo más:

```{r}
#| label: fig-descriptivos
#| fig-cap: "Frecuencias Cohesión barrial"
proc_data %>% dplyr::select(ideal, integracion, identificacion, pertenencia) %>% 
  sjPlot::plot_stackfrq()+
  theme(legend.position = "bottom")

```

En la @fig-descriptivos podemos ver la distribución de las variables de cohesión barrial, donde se puede observar que más del 65% de la muestra está de acuerdo o totalmente de acuerdo con las afirmaciones indicadas.

### Asociación de variables

Podemos ver la asociación de todas las variables, como lo muestra la @cor-complete

```{r}
M <- cor(proc_data, use = "complete.obs") # Usar solo casos con observaciones completas
```

```{r}
#| label: cor-complete
#| fig-cap: "Cohesión variables elsoc 2016"


corrplot.mixed(M)
```

o podemos ver específicamente la asociación de las variables de cohesión barrial

```{r}
#| label: fig-cor-cohesion
#| fig-cap: "Correlación Cohesión barrial"

M2 <- cor(dplyr::select(proc_data, ideal, integracion, identificacion, pertenencia), use = "complete.obs")
corrplot.mixed(M2)
```

La @fig-cor-cohesion muestra que la asociación de las cuatro variables de cohesión barrial es alta y positiva, según Cohen (1998). En general el tamaño de efecto varía entre 0.58 la más baja y 0.69 la más alta.

### Construcción de escala

```{r}
psych::alpha(dplyr::select(proc_data, ideal, integracion, identificacion, pertenencia))
```

La consistencia interna de una posible escala entre estos cuatro ítems es de 0.87, lo que representa una alta consistencia interna. Si quitaramos alguno de estos ítems la consistencia interna solo bajaría, así que podemos construir una escala con los cuatro ítems.

```{r}
proc_data <- proc_data %>% 
  rowwise() %>% 
  mutate(cohesion_barrial = sum(ideal, integracion, identificacion, pertenencia))
summary(proc_data$cohesion_barrial)
```

y la podemos visualizar en un gráfico:

```{r}
#| label: fig-plot-cohesion
#| fig-cap: "Histograma Cohesión barrial"


ggplot(proc_data, aes(x = cohesion_barrial)) +
  geom_histogram(binwidth=0.6, colour="black", fill="yellow") +
  theme_bw() +
  xlab("Cohesión barrial") +
  ylab("Cantidad")
```

La @fig-plot-cohesion muestra la distribución de la nueva escala de Cohesión Barrial que construimos. En general, la mayor concentración de casos está en la categoría 12 y que sumado a un promedio de 10.33 según los descriptivos anteriores, podríamos afirmar que la cohesión barrial en Chile es alta.

# Subir a github

Una vez que tenemos nuestro cambios hechos, vamos a Github desktop y seguimos los siguientes pasos:

![](../files/img/upload-github.png)
