---
title: "Práctico 3. Operacionalización de variables"
subtitle: "R data analisis"
linktitle: "Práctico 3: Operacionalización"
date: "2024-03-26"
lang: es
---

# Presentación

## Objetivo de la práctica

El desarrollo de esta guía tiene por objetivo revisar algunos procedimientos básicos de la preparación de datos con R, que son necesarios para luego poder analizar e interpretar los datos.

Por temas de orden y reproducibilidad, en este curso vamos a separar en **dos momentos** el trabajo con datos, y dos archivos de código correspondientes:

  - **Preparación** corresponde a lo que se conoce generalmente como "limpieza", es decir, realizar las modificaciones necesarias para poder efectuar los análisis. Estas modificaciones previas al análisis son necesarias ya que los datos originales con los que se va a trabajar en general no vienen perfectamente adaptados a los análisis que se quieren hacer. Por lo tanto, en cuanto a datos también hacemos la distinción entre datos originales y datos preparados (o procesados).

  - **Análisis**: se relaciona tanto con análisis descriptivos asociados a las preguntas de investigación y como también modelamiento de datos para contrastar hipótesis de investigación.

#### Los procesos de preparación y análisis vinculados a datos y resultados se presentan en el siguiente esquema:{#flujo}

![](images/produccion.png)

Tanto la preparación como el análisis (que son parte del concepto más general de procesamiento) quedan registrados cada uno en un _archivo de código_.

<span class="sidenote">**Archivo de código R**: archivo con extensión .R donde se almacena el código de análisis. Para generarlo desde RStudio: _File > New File > R Script_ (o ctrl+shift+N), y para grabarlo  _File > Save (o ctrl+s)_, y darle nombre la primera vez (recordar: sin tilde ni ñ, y evitar espacios) </span>

El documento de **código de preparación** posee 5 partes, más una sección de identificación inicial:

0. Identificación y descripción general: Título, autor(es), fecha, información breve sobre el contenido del documento
1. Librerías: cargar librerías a utilizar
2. Datos: carga de datos
3. Selección de variables a utilizar
4. Procesamiento de variables: en este punto, por cada variable se realiza lo siguiente:
    a. Descriptivo básico
    b. Recodificación: datos perdidos y valores (en caso de ser necesario)
    c. Etiquetamiento: de variable y valores (en caso de ser necesario)
    e. Otros ajustes


5. Generación de base de datos preparada para el análisis.

<div class="alert alert-info">
**De rutas, estructura de carpetas y otros **

- **Encontrando la ruta a carpeta local**: lo más fácil es crear la carpeta donde se desean guardar los datos desde el administrador de archivos del computador. Luego, posicionarse con el cursor sobre la carpeta y seleccionar "Propiedades", en la ventana emergente debería aparecer la ruta hacia la carpeta en "Ubicación". Copiar esa ruta y agregar al final el nombre de la carpeta (separada por slash)

- **Sobre los "slashes" (`\` o `/`)**: en la ruta las carpetas y el archivo final aparecen separados por slashes, que según el sistema utilizado pueden ser _slash_ (`/`) o _backslash_ (`\`). En R por defecto se usa _slash_, pero en Windows _backslash_, por lo que si se usa Windows hay que reemplazarlos por _backslash_ o también puede ser por un doble _slash_ (`//`).

- Por temas de compatibilidad general, en las rutas se recomienda evitar tildes, eñes, espacios, mayúsculas y guiones bajos (_).

- **Estructura de carpetas**: para mantener el orden se sugiere seguir un protocolo de estructura de carpetas de proyecto, para lo que recomendamos el protocolo [IPO](https://lisa-coes.com/ipo-protocol/), y que se adapta al flujo de trabajo presentado anteriormente. Básicamente son tres carpetas: **input**, **procesamiento**, **output**. En la carpeta input crear la subcarpeta data-orig para guardar datos originales, y data-proc para los procesados. En procesamiento se guardan los archivos de código y en output las tablas y los gráficos.

![](../files/img/ipo-hex.png)


<div>




Al final de esta práctica la idea es que cada un_ elabore y entienda su propio documento de preparación de datos.

En el ejemplo vamos a procesar variables de confianza en instituciones políticas y variables de caracterización sociodemográfica utilizando los datos de la encuesta [Latinobarómetro](https://www.latinobarometro.org/lat.jsp) .

## Antecedentes de los datos a utilizar

Latinobarómetro es un estudio de opinión pública que aplica anualmente alrededor de 20.000 entrevistas en 18 países de América Latina representando a más de 600 millones de habitantes

  "Los gobiernos latinoamericanos, que venían en declive junto con sus democracias desde inicios de la década de 2010, como reflejan los datos de Latinobarómetro, llegaron a fines de 2018 al annus horribilis con la caída de Nicaragua y Venezuela desde su condición de democracias para entrar en la categoría de autocracias y dictadura. 

  De los hiperpresidentes de la primera década del siglo con altos niveles de crecimiento en todos los países y altos niveles de aprobación, la región pasó en la segunda década a los subpresidentes, con una baja en aprobación de gobierno a la mitad, en menos de 10 años. Este rechazo al desempeño de las elites gobernantes, indica su fracaso a fines de 2019 en varios países de la región." (Latinobarómetro, informe 2021, p. 7)


El presente ejercicio tiene por objetivo el procesar los datos para obtener las variables relevantes para el estudio de la **Confianza en instituciones políticas**, entendida como el grado en que los individuos confian en distintas instituciones políticas a nivel nacional, como el gobierno, la justicia, los partidos políticos, etc. Para ello, junto con variables de confianza, consideraremos también variables de estatus (educación), y variables de caracterización sociodemográfica (sexo y edad).

# Preparación de datos Latinobarómetro 2020

## 1. Librerías principales (de R) a utilizar en el análisis{#librerias}

Como sabemos, la lógica de R es instalar librerías (solo 1 vez, con `install.packages("librería")`), y luego cargarlas cada vez que es necesario usarlas (con `library(librería)`). El problema de esto es que a veces no se sabe claramente qué librerías están instaladas y cuales no, lo que va a arrojar error al cargarlas. Y, como sucede en R, existe una librería para solucionar este problema que se llama `pacman` (package manager). Lo que hace `pacman` es cargar la librería, y si no está instalada, la instala y la carga:


Para utilizar la primera vez (si es que no está instalada):

```{r eval=FALSE}
install.packages("pacman")
```

Y en adelante, las librerías se cargan así <span class="sidenote"> pacman::p_load(libreria1,libreria2,libreriaX) </span>:


```{r}
pacman::p_load(dplyr, sjmisc, car, sjlabelled, stargazer, haven)
```


Para esta sesión vamos a utilizar Las librerías que vamos a utilizar son:

- `dplyr`: ajuste general de datos
- `sjmisc`: descripción y exploración de base de datos
- `car`: principalmente la función `recode` para recodificar/agrupar valores de variable
- `stargazer`: para tabla descriptiva


## 2. Cargar base de datos

**Ajustar espacio de trabajo**

Previo a la carga de nuestra base de datos, se recomienda ejecutar los siguientes comandos:

```{r}
rm(list=ls())       # borrar todos los objetos en el espacio de trabajo
options(scipen=999) # valores sin notación científica
```

La función `rm(list=ls())` permite comenzar con un espacio de trabajo (environment) vacío y sin otros objetos. Así también, la función `options(scipen=999)` desactiva la notación científica, es decir, veremos los valores numéricos con todos sus decimales.

**Datos**

Las bases de datos se pueden cargar de un archivo local o en línea. Para este caso utilizaremos un archivo en línea que viene en formato RData: **latinobarometro2020.RData**. <span class="sidenote">**Abrir bases de datos en otros formatos**: Los formatos mas comunes en que se almacenan las bases de datos son .dta (Stata),  .sav (Spss) y RData (R). Para abrir desde R utlilizamos la librería `haven` y sus funciones read_dta y read_sav según corresponda. Ej: `datos <- read_dta("base_casen.dta")`. Recordar antes instalar/cargar la librería: `pacman::p_load(haven)`   </span>

```{r eval=FALSE, }
#cargamos la base de datos desde internet
load(url("https://github.com/Kevin-carrasco/metod1-MCS/raw/main/files/data/external_data/latinobarometro2020.RData"))
```

o de manera local:

```{r}
latinobarometro2020 <- read_dta("../files/data/external_data/latinobarometro2020.dta", encoding = "UTF-8")
```


La base de datos aparece como un objeto en nuestro espacio de trabajo, con el nombre original con la que fue guardada (latinobarometro2020):

![](images/basecargada.png)

Realizamos un chequeo básico de la lectura de datos: nombres de las variables y tamaño de la base en términos de casos y variables (en este ejemplo, `r dim(latinobarometro2020)` ).

```{r}
dim(latinobarometro2020) # dimension de la base
```
Y si se quiere revisar en formato de planilla de datos:

```{r}
View(latinobarometro2020)
```

## 3. Selección de variables a utilizar

Este paso consiste en crear un subset reducido de datos que contenga solo las variables de interés. Para ello:

1. Se identifica el nombre de las variables que registran la información de preguntas o items del instrumento: esto aparece en el libro de códigos y/o en el cuestionario, o también se puede hacer buscando en la base de datos mediante alguna palabra clave asociada a la pregunta. Por ejemplo, si queremos buscar variables asociadas a educación, utilizamos la función `find_var` (de `sjmisc`, librería que cargamos en el paso 1), que nos entrega nombre de la variable en columna var.name. Por ejemplo, si buscamos alguna variable asociada al concepto _Confianza_:

```{r}
find_var(data = latinobarometro2020,"Confianza")
```

Nos informa que hay una serie de variables relacionadas con confianza interpersonal y con instituciones. Probemos con la variable `p13st_e`.


Mediante la función `select` de `dplyr`, seleccionamos cada una de nuestras variables de interés y creamos una nueva base con el nombre `proc_data`, donde "proc" hace referencia a base procesada:

```{r}
proc_data <- latinobarometro2020 %>% select(p13st_e, # Confianza en el Gobierno
                          p13st_d, # Confianza en el congreso
                          p13st_f, # Confianza en el Poder Judicial
                          p13st_g, # Confianza en los partidos políticos
                          reeduc_1,# nivel educacional
                          sexo,# sexo
                          edad,# edad
                          idenpa) # pais 

# Comprobar
names(proc_data)
```

Mediante el comando `get_label` obtenemos el atributo label de las variables.

```{r}
sjlabelled::get_label(proc_data)
```

Podemos ver que son largas o con códigos poco informativos, por lo tanto, es necesario cambiarlas por etiquetas más cortas y de fácil identificación.

```{r echo=FALSE}
load(url("https://github.com/Kevin-carrasco/metod1-MCS/raw/main/files/data/external_data/latinobarometro2020.RData"))
proc_data <- latinobarometro2020 %>% select(p13st_e, # Confianza en el Gobierno
                          p13st_d, # Confianza en el congreso
                          p13st_f, # Confianza en el Poder Judicial
                          p13st_g, # Confianza en los partidos políticos
                          reeduc_1,# nivel educacional
                          sexo,# sexo
                          edad,# edad
                          idenpa) # pais 
```


Para facilitar el análisis, vamos a filtrar la base de datos para quedarnos solo con los casos de Chile. Para esto utilizamos la función `filter` de `dplyr`. Si revisamos el libro de códigos, el identificador de Chile es 152

```{r}
proc_data <- proc_data %>% dplyr::filter(idenpa==152)
```

## 4. Procesamiento de variables

Para el procesamiento de cada variable se seguirá el siguiente flujo de trabajo:

a. Descriptivo general
b. Recodificación: de casos perdidos y otros valores (en caso necesario)
c. Etiquetado: cambio de nombres de variables y valores (en caso necesario)
d. Otros ajustes

Y se recomienda también un descriptivo final para revisar que el procesamiento de cada variable está ok.

### 4.1 Confianza en el Gobierno

En Latinobarómetro, lass variables que permiten medir la Confianza en instituciones políticas en Chile son las siguientes:

* [`p13st_e`]: "P13ST.E Confianza en el Gobierno" (1 = Mucha; 4 = Ninguna)
* [`p13st_d`]: "P13ST.D Confianza en el Congreso" (1 = Mucha; 4 = Ninguna)
* [`p13st_f`]: "P13ST.F Confianza en el Poder Judicial" (1 = Mucha; 4 = Ninguna)
* [`p13st_g`]: "P13ST.G Confianza en los Partidos Políticos" (1 = Mucha; 4 = Ninguna) 

_a. Descriptivo_

Para los descriptivos se utilizará la función `frq`, de la librería `sjmisc`:

```{r}
frq(proc_data$p13st_e)
```

En esta variable vemos valores asociados a la opción "No contesta" (-2) y "No sabe" (-1), que corresponde definirlos como casos perdidos (en el caso de R, como casos NA). El resto de los valores y etiquetas se encuentran en un orden contraintuitivo (mayor valor indica menos confianza), así que en la recodificiación nos haremos cargo de los casos perdidos y de reordenar las categorías.

_b. Recodificación_

Para recodificar utilizamos la función `recode`, de la librería `car`

```{r}
proc_data$p13st_e <- recode(proc_data$p13st_e, "c(-2,-1)=NA")
proc_data$p13st_d <- recode(proc_data$p13st_d, "c(-2,-1)=NA")
proc_data$p13st_f <- recode(proc_data$p13st_f, "c(-2,-1)=NA")
proc_data$p13st_g <- recode(proc_data$p13st_g, "c(-2,-1)=NA")
```

**nota**: con la función `set_na` de la librería `sjmisc` podemos recodificar toda la base de datos con un solo código, pero debemos estar completamente segur-s de que estos valores no tienen otra categoría asociada en otra variable.

```{r eval=FALSE}
proc_data <- proc_data %>% set_na(., na = c(-2, -1))
```

Para reordenar las categorías volvemos a utilizar la función `recode`, de la librería `car`

```{r}
proc_data$p13st_e <- recode(proc_data$p13st_e, "1=3; 2=2; 3=1; 4=0")
proc_data$p13st_d <- recode(proc_data$p13st_d, "1=3; 2=2; 3=1; 4=0")
proc_data$p13st_f <- recode(proc_data$p13st_f, "1=3; 2=2; 3=1; 4=0")
proc_data$p13st_g <- recode(proc_data$p13st_g, "1=3; 2=2; 3=1; 4=0")
```

_c - Etiquetado_

Vamos a dar un nombre más sustantivo a las variables con la función `rename`, de la librería `dplyr`:

```{r}
proc_data <- proc_data %>% rename("conf_gob"=p13st_e, # Confianza en el gobierno
                                  "conf_cong"=p13st_d, # Confianza en el congreso
                                  "conf_jud"=p13st_f, # Confianza en el Poder Judicial
                                  "conf_partpol"=p13st_g) # Confianza en los partidos políticos 

```


Además de cambiar el nombre, queremos cambiar las etiquetas de las variables.

```{r}
proc_data$conf_gob <- set_label(x = proc_data$conf_gob,label = "Confianza: Gobierno")
get_label(proc_data$conf_gob)

proc_data$conf_cong  <- set_label(x = proc_data$conf_cong, label = "Confianza: Congreso")
get_label(proc_data$conf_cong)

proc_data$conf_jud  <- set_label(x = proc_data$conf_jud, label = "Confianza: Poder judicial")
get_label(proc_data$conf_jud)

proc_data$conf_partpol  <- set_label(x = proc_data$conf_partpol, label = "Confianza: Partidos politicos")
get_label(proc_data$conf_partpol)
```

_d. Otros ajustes_

Para este caso vamos a crear una variable que sea la suma de los cuatro items de confianza.

```{r}
proc_data$conf_inst <- (proc_data$conf_gob+proc_data$conf_cong+proc_data$conf_jud+proc_data$conf_partpol)
summary(proc_data$conf_inst)
```

```{r}
get_label(proc_data$conf_inst)
```

Vemos que una etiqueta de la variable anterior.

```{r}
proc_data$conf_inst  <- set_label(x = proc_data$conf_inst, label = "Confianza en instituciones")
```


_Revisión final_

Nuevamente un descriptivo de cada variable para confirmar que el procesamiento está ok:

```{r}
frq(proc_data$conf_gob)
frq(proc_data$conf_cong)
frq(proc_data$conf_inst)
```

Vemos que los valores (labels) de cada categoría de las primeras variables que recodificamos no se corresponden con el nuevo valor. Para re-etiquetar valores usamos la función `set_labels`, de la librería `sjlabelled`

```{r}
proc_data$conf_gob <- set_labels(proc_data$conf_gob,
            labels=c( "Ninguna"=0,
                      "Poca"=1,
                      "Algo"=2,
                      "Mucha"=3))

proc_data$conf_cong <- set_labels(proc_data$conf_cong,
            labels=c( "Ninguna"=0,
                      "Poca"=1,
                      "Algo"=2,
                      "Mucha"=3))

proc_data$conf_jud <- set_labels(proc_data$conf_jud,
            labels=c( "Ninguna"=0,
                      "Poca"=1,
                      "Algo"=2,
                      "Mucha"=3))

proc_data$conf_partpol <- set_labels(proc_data$conf_partpol,
            labels=c( "Ninguna"=0,
                      "Poca"=1,
                      "Algo"=2,
                      "Mucha"=3))
```

y volvemos a revisar

```{r}
frq(proc_data$conf_gob)
frq(proc_data$conf_cong)
```

#### 4.2. Educación

* [`reeduc_1`] = REEDUC.1 Nivel de estudios alcanzado - Entrevistado (recodificado)

_a. Descriptivo_

```{r}
frq(proc_data$reeduc_1)
```

_b. Recodificación_

- Vemos que no hay datos perdidos

- Valores

Para hacer más fácil el análisis, recodificamos en tres categorías (en este caso decisión arbitraria. Se debería tener una razón teórica para recodificar)

```
1.  Analfabeto                                =   Educacion basica    =   1
2   Básica incompleta                         =   Educacion basica    =   1
3.  Básica completa                           =   Educacion basica    =   1
4.  Secundaria, media, técnica incompleta     =   Educacion media     =   2
5.  Secundaria, media, técnica completa       =   Educacion media     =   2
6.  Superior incompleta                       =   Educacion superior  =   3
7.  Superior completa                         =   Educacion superior  =   3

```
```{r}
# recodificacion usando funcion 'recode' de la libreria car
proc_data$reeduc_1 <- car::recode(proc_data$reeduc_1, "c(1,2,3)=1; c(4,5)=2; c(6,7)=3")
```

Comprobar con un nuevo descriptivo:

```{r}
frq(proc_data$reeduc_1)
```

Se observa que los valores coinciden con la recodificación (los casos se acumulan entre las categorías 1 y 3), pero las etiquetas ahora no coinciden; se soluciona en el siguiente paso.

_c. Etiquetado_

Para re-etiquetar valores usamos la función `factor`, de R base. Con esta función aprovechamos de transformar la variable educación en una variable categórica, que es lo que corresponde para una variable ordinal.

```{r}
proc_data$reeduc_1 <- factor(proc_data$reeduc_1,
                             labels = c("Educacion basica", "Educacion media", "Educacion superior"),
                             levels = c(1, 2, 3))
```

Luego renombramos la variable con un nombre más sustantivo

```{r}
proc_data <- rename(proc_data,"educacion"=reeduc_1)
```

Además de cambiar el nombre, queremos cambiar la etiqueta de la variable.

```{r}
get_label(proc_data$educacion)
proc_data$educacion <- set_label(x = proc_data$educacion,label = "Educación")
```


### 4.3. Sexo

* [`sexo`]	=	SEXO Sexo

_a. Descriptivo_

```{r}
frq(proc_data$sexo)
```

_b. Recodificación_

En general esta variable no tiene problemas de casos perdidos ni de etiquetas, pero de todas maneras vamos a hacer un cambio de acuerdo a convenciones en análisis de datos, donde por lo general hombres tienen valor 0 y mujeres 1:

```{r}
proc_data$sexo <- car::recode(proc_data$sexo, "1=0;2=1")
```

_c. Etiquetado_

Y ahora cambiamos las etiquetas de acuerdo a la recodificación anterior:

```{r}
proc_data$sexo <- factor(proc_data$sexo,
            labels=c( "Hombre",
                      "Mujer"),
            levels=c(0,1))
```


También queremos cambiar la etiqueta de la variable.

```{r}
get_label(proc_data$sexo)
proc_data$sexo <- set_label(x = proc_data$sexo,label = "Sexo")
```

Revisar con un nuevo descriptivo:

```{r}
frq(proc_data$sexo)
```

### 4.4 Edad

* [`edad`]	=	EDAD Edad.


_a. Descriptivo_

```{r}
frq(proc_data$edad)
```

_b. Recodificación_: no es necesario en este caso

_c. Etiquetado_

Cambio la etiqueta de la variable.

```{r}
get_label(proc_data$edad)
proc_data$edad <- set_label(x = proc_data$edad,label = "Edad")
```

## 5. Generación de base de datos procesada para el análisis

Antes de guardar la base procesada, revisamos nuevamente todas las variables con una tabla descriptiva general mediante la función `stargazer` (de la librería homónima)

Primero vamos a reformatear el objeto proc_data como base de datos (as.data.frame), paso necesario para que sea reconocido como tal por `stargazer`


```{r}
proc_data <-as.data.frame(proc_data)
stargazer(proc_data, type="text")
```
<div class="alert alert-info">
Si se desea modificar las columnas que aparecen en la tabla se puede ocupar la opción `summary.stat`, donde se pueden especificar:

- "max" maximum
- "mean" mean
- "median" median
- "min" minimum
- "n" number of observations
- "p25" 25th percentile
- "p75" 75th percentile
- "sd" standard deviation

Por ejemplo, si quiero una tabla solo con promedio, n, sd y p75: `stargazer(data, type="text", summary.stat = c("mean", "n", "sd", "p75"))`

</div>

- Guardar base de datos procesada: en carpeta local <span class="sidenote">La ruta hacia su carpeta local si está trabajando en windows debería ser algo como "C:/Users/Lenovo/Clases/y aquí nombre del archivo a grabar</span>

El comando para guardar es `save`:

```{r eval=FALSE}
save(proc_data,file = "[ruta hacia carpeta local en su computador]/ELSOC_ess_merit2016.RData")
```

En este caso, seguimos una estructura de carpetas de datos, separando en una carpeta los datos originales, y en otra (proc) los datos procesados:


```{r eval=FALSE}
save(proc_data,file = "files/data/latinobarometro_proc.RData")
```


## Descriptivos básicos de las variables

Podemos conocer ciertas medidas de tendencia central utilizando algunas funciones de `dplyr`

### Media por grupos

```{r}
proc_data %>% dplyr::group_by(sexo) %>% summarise(mean(conf_inst, na.rm=TRUE))
```

```{r}
proc_data %>% dplyr::group_by(educacion) %>% summarise(mean(conf_inst, na.rm=TRUE))
```

### Representación

```{r}
library(sjPlot)
sjt.xtab(proc_data$educacion, proc_data$conf_inst, encoding = "UTF-8")
```
